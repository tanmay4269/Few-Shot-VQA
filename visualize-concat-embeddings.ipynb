{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Concatinated Image and Question Embeddings \n",
    "I'll be using DINO vision encoder and SBERT question encoder to check t-SNE plots on VQA-v2 and VQA-Abstract datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT with sum of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get sentence embeddings\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = bert(**inputs)\n",
    "        embedding = outputs.last_hidden_state.squeeze().sum(dim=0).numpy()\n",
    "        \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "sentences = [\n",
    "    'i loves mangos',\n",
    "    'i hates mangos',\n",
    "    \n",
    "    'i loves apples',\n",
    "    'i hates apples',\n",
    "\n",
    "    'i loves bananas',\n",
    "    'i hates bananas',\n",
    "    \n",
    "    'i loves strawberries',\n",
    "    'i hates strawberries',\n",
    "    \n",
    "    'i loves blueberries',\n",
    "    'i hates blueberries',\n",
    "\n",
    "    'i love people',\n",
    "    'i hate people',\n",
    "    \n",
    "    'i love dogs',\n",
    "    'i hate dogs',\n",
    "    \n",
    "    'i love cats',\n",
    "    'i hate cats',\n",
    "    \n",
    "    'i love monkeys',\n",
    "    'i hate monkeys',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for all sentences\n",
    "embeddings = np.array([get_sentence_embedding(sentence) for sentence in sentences])\n",
    "\n",
    "# Apply t-SNE\n",
    "# perplexity = min(30, len(embeddings) - 1)\n",
    "perplexity = 4\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "tsne_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot the t-SNE reduced embeddings\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    plt.scatter(tsne_embeddings[i, 0], tsne_embeddings[i, 1])\n",
    "    plt.text(tsne_embeddings[i, 0] + 0.1, tsne_embeddings[i, 1], sentence, fontsize=9)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('t-SNE Visualization of BERT Sentence Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: I'd expect fruits and non fruits to be on different sides but it seems to be a mess in here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBERT: Better sentence embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = sbert.encode(sentences)\n",
    "\n",
    "# Apply t-SNE\n",
    "# perplexity = min(30, len(embeddings) - 1)\n",
    "perplexity = 4\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "tsne_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot the t-SNE reduced embeddings\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    plt.scatter(tsne_embeddings[i, 0], tsne_embeddings[i, 1])\n",
    "    plt.text(tsne_embeddings[i, 0] + 0.1, tsne_embeddings[i, 1], sentence, fontsize=9)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('t-SNE Visualization of BERT Sentence Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: clearly the seperation is much better although I'd expect better seperation for 'love' and 'hate'. Apparently the model thinks they are more similar than our monkey brains think :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINO vision encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to get image embeddings\n",
    "def get_image_embedding(image_path, processor, model):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of images\n",
    "image_dir = \"images/cats_dogs\"\n",
    "\n",
    "# Load pre-trained DINOv2 model and processor\n",
    "processor_small = AutoImageProcessor.from_pretrained('facebook/dinov2-small')\n",
    "dino_small = AutoModel.from_pretrained('facebook/dinov2-small')\n",
    "\n",
    "processor_base = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "dino_base = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "def small_embeddings():\n",
    "    # Get embeddings and file names\n",
    "    embeddings = []\n",
    "    file_names = []\n",
    "    for file_name in os.listdir(image_dir):\n",
    "        if file_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(image_dir, file_name)\n",
    "            embeddings.append(get_image_embedding(image_path, processor_small, dino_small))\n",
    "            file_names.append(os.path.splitext(file_name)[0])\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def base_embeddings():\n",
    "    # Get embeddings and file names\n",
    "    embeddings = []\n",
    "    file_names = []\n",
    "    for file_name in os.listdir(image_dir):\n",
    "        if file_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(image_dir, file_name)\n",
    "            embeddings.append(get_image_embedding(image_path, processor_base, dino_base))\n",
    "            file_names.append(os.path.splitext(file_name)[0])\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def plot_tsne(embeddings, perplexity):\n",
    "    # Apply t-SNE\n",
    "    # perplexity = min(30, len(embeddings) - 1)\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    tsne_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot the t-SNE reduced embeddings\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        plt.scatter(tsne_embeddings[i, 0], tsne_embeddings[i, 1])\n",
    "        plt.text(tsne_embeddings[i, 0] + 0.1, tsne_embeddings[i, 1], file_name, fontsize=9)\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.title('t-SNE Visualization of Image Embeddings')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = small_embeddings()\n",
    "plot_tsne(embeddings, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = base_embeddings()\n",
    "plot_tsne(embeddings, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: I'd need to check more on image embedding quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision & Text Embeddings Visualised Together (concat)\n",
    "## VQA-V2\n",
    "### 1. Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = 'data/vqa-v2/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    vqa_v2_data = json.load(file)\n",
    "\n",
    "print(vqa_v2_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_questions = vqa_v2_data['questions']\n",
    "print(v2_questions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vision and squence encoders (coz i dont wanna run cells above :P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "processor_small = AutoImageProcessor.from_pretrained('facebook/dinov2-small')\n",
    "dino_small = AutoModel.from_pretrained('facebook/dinov2-small')\n",
    "\n",
    "processor_base = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "dino_base = AutoModel.from_pretrained('facebook/dinov2-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_root, image_id, processor, model, show_img=False):\n",
    "    image_id = str(image_id).zfill(6)\n",
    "    image_path = f'{image_root}{image_id}.jpg'\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if show_img:\n",
    "        tiny_image = image.resize((64,64))\n",
    "        tiny_image.show()\n",
    "    \n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "def plot_tsne(embeddings, plot_labels, perplexity):\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    tsne_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for i, plot_label in enumerate(plot_labels):\n",
    "        plt.scatter(tsne_embeddings[i, 0], tsne_embeddings[i, 1])\n",
    "        plt.text(tsne_embeddings[i, 0] + 0.1, tsne_embeddings[i, 1], plot_label, fontsize=9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root = 'data/vqa-v2/val2014/val2014/COCO_val2014_000000'\n",
    "processor, image_encoder = processor_small, dino_small\n",
    "# processor, image_encoder = processor_base, dino_base\n",
    "\n",
    "v2_image_embeddings, v2_question_embeddings = [], []\n",
    "v2_image_labels, v2_question_labels = [], []\n",
    "for i, question in enumerate(v2_questions[:1000]):\n",
    "    image_id = question['image_id']\n",
    "    v2_image_labels.append(image_id)\n",
    "    \n",
    "    q = question['question']\n",
    "    q_id = question['question_id']\n",
    "    v2_question_labels.append(q)\n",
    "    v2_question_embeddings.append(sbert.encode(q))\n",
    "\n",
    "    v2_image_embeddings.append(get_image_embedding(image_root, image_id, processor, image_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_image_embeddings = np.array(v2_image_embeddings)\n",
    "v2_question_embeddings = np.array(v2_question_embeddings)\n",
    "\n",
    "empty_labels = [''] * len(v2_image_labels)\n",
    "# plot_tsne(v2_image_embeddings, v2_image_labels, 4)\n",
    "# plot_tsne(v2_question_embeddings, v2_image_labels, 6)\n",
    "\n",
    "plot_tsne(v2_image_embeddings, empty_labels, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(v2_question_embeddings, empty_labels, 16)\n",
    "plot_tsne(v2_question_embeddings, empty_labels, 32)\n",
    "plot_tsne(v2_question_embeddings, empty_labels, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_cat = np.concatenate((v2_image_embeddings, v2_question_embeddings), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(v2_cat, empty_labels, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQA-Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = 'data/vqa-abstract/questions_train/OpenEnded_abstract_v002_train2015_questions.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    vqa_abstract_data = json.load(file)\n",
    "\n",
    "abs_questions = vqa_abstract_data['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_root, image_id, processor, model, show_img=False):\n",
    "    image_id = str(image_id).zfill(5)\n",
    "    image_path = f'{image_root}{image_id}.png'\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if show_img:\n",
    "        tiny_image = image.resize((64,64))\n",
    "        tiny_image.show()\n",
    "    \n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root = 'data/vqa-abstract/img_train/abstract_v002_train2015_0000000'\n",
    "processor, image_encoder = processor_small, dino_small\n",
    "# processor, image_encoder = processor_base, dino_base\n",
    "\n",
    "abs_image_embeddings, abs_question_embeddings = [], []\n",
    "abs_image_labels, abs_question_labels = [], []\n",
    "for i, question in enumerate(abs_questions[:1000]):\n",
    "    image_id = question['image_id']\n",
    "    abs_image_labels.append(image_id)\n",
    "    # print(i, image_id)\n",
    "    \n",
    "    q = question['question']\n",
    "    q_id = question['question_id']\n",
    "    abs_question_labels.append(q)\n",
    "    abs_question_embeddings.append(sbert.encode(q))\n",
    "\n",
    "    # print(f'image_id: {image_id}\\t {q_id} - {q}')\n",
    "    abs_image_embeddings.append(get_image_embedding(image_root, image_id, processor, image_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_image_embeddings = np.array(abs_image_embeddings)\n",
    "abs_question_embeddings = np.array(abs_question_embeddings)\n",
    "\n",
    "empty_labels = [''] * len(abs_image_labels)\n",
    "\n",
    "# plot_tsne(abs_image_embeddings, abs_image_labels, 4)\n",
    "# plot_tsne(abs_question_embeddings, abs_image_labels, 6)\n",
    "\n",
    "plot_tsne(abs_image_embeddings, empty_labels, 2)\n",
    "plot_tsne(abs_image_embeddings, empty_labels, 4)\n",
    "plot_tsne(abs_image_embeddings, empty_labels, 8)\n",
    "plot_tsne(abs_image_embeddings, empty_labels, 16)\n",
    "plot_tsne(abs_image_embeddings, empty_labels, 32)\n",
    "plot_tsne(abs_image_embeddings, empty_labels, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(abs_question_embeddings, empty_labels, 4)\n",
    "plot_tsne(abs_question_embeddings, empty_labels, 8)\n",
    "plot_tsne(abs_question_embeddings, empty_labels, 16)\n",
    "plot_tsne(abs_question_embeddings, empty_labels, 32)\n",
    "plot_tsne(abs_question_embeddings, empty_labels, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_cat = np.concatenate((abs_image_embeddings, abs_question_embeddings), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(abs_cat, empty_labels, 4)\n",
    "plot_tsne(abs_cat, empty_labels, 8)\n",
    "plot_tsne(abs_cat, empty_labels, 16)\n",
    "plot_tsne(abs_cat, empty_labels, 32)\n",
    "plot_tsne(abs_cat, empty_labels, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting both on the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = np.concatenate((v2_image_embeddings, abs_image_embeddings), axis=0)\n",
    "question_embeddings = np.concatenate((v2_question_embeddings, abs_question_embeddings), axis=0)\n",
    "cat = np.concatenate((v2_cat, abs_cat), axis=0)\n",
    "\n",
    "labels = ['V'] * len(v2_image_embeddings) + ['A'] * len(abs_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(embeddings, plot_labels, perplexity):\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    tsne_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for i, plot_label in enumerate(plot_labels):\n",
    "        plt.scatter(tsne_embeddings[i, 0], tsne_embeddings[i, 1], color='blue' if plot_label == 'V' else 'red')\n",
    "        plt.text(tsne_embeddings[i, 0] + 0.1, tsne_embeddings[i, 1], '', fontsize=9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(image_embeddings, labels, 4)\n",
    "plot_tsne(image_embeddings, labels, 8)\n",
    "plot_tsne(image_embeddings, labels, 16)\n",
    "plot_tsne(image_embeddings, labels, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(question_embeddings, labels, 4)\n",
    "plot_tsne(question_embeddings, labels, 8)\n",
    "plot_tsne(question_embeddings, labels, 16)\n",
    "plot_tsne(question_embeddings, labels, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(cat, labels, 4)\n",
    "plot_tsne(cat, labels, 8)\n",
    "plot_tsne(cat, labels, 16)\n",
    "plot_tsne(cat, labels, 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
