{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T08:51:42.030483Z",
     "start_time": "2024-08-26T08:51:41.905212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_save_path: ./weights/raw/name=DANN__n_classes=10__v2_samples_per_answer=300__abs_samples_per_answer=150__source_domain=v2__base_lr=0.001__Tf6WBM3f.pth\n",
      "Number of Common Labels = 25 | n_classes = 10\n",
      "Labels: ['man', 'yes', 'no', 'brown', 'dog', '2', '1', 'gray', '3', 'food']\n",
      "V2: \tTrain size = 2400             | Val size = 600 | Total = 3000\n",
      "Abs: \tTrain size = 1200             | Val size = 300 | Total = 1500\n",
      "--------------------\n",
      "\t Iter [0/24]\t Loss: 1.543581\n",
      "\t Iter [6/24]\t Loss: 1.515600\n",
      "\t Iter [12/24]\t Loss: 1.775328\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m trainer \u001b[38;5;241m=\u001b[39m DA_Trainer(cfg, vqa_v2, vqa_abs)\n\u001b[1;32m     67\u001b[0m v2_ckpt_path \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights_save_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 69\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# abs\u001b[39;00m\n\u001b[1;32m     72\u001b[0m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_domain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabs\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/workspace/trainer.py:612\u001b[0m, in \u001b[0;36mDA_Trainer.train\u001b[0;34m(self, show_plot, optuna_trial, comet_expt)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2_train_dataloader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabs_train_dataloader\n\u001b[1;32m    609\u001b[0m )\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2_val_dataloader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabs_val_dataloader)\n\u001b[0;32m--> 612\u001b[0m label_loss, domain_loss, total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    614\u001b[0m     (\n\u001b[1;32m    615\u001b[0m         eval_loss,\n\u001b[1;32m    616\u001b[0m         v2_accuracy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m         domain_accuracy,\n\u001b[1;32m    620\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_epoch()\n",
      "File \u001b[0;32m/workspace/trainer.py:508\u001b[0m, in \u001b[0;36mDA_Trainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m domain_running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    506\u001b[0m total_running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 508\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (\n\u001b[1;32m    509\u001b[0m         (v2_i_tokens, v2_q_tokens, v2_label),\n\u001b[1;32m    510\u001b[0m         (abs_i_tokens, abs_q_tokens, abs_label),\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_alpha(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_batches)\n\u001b[1;32m    515\u001b[0m     (label_loss, domain_loss, total_loss), _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_input(\n\u001b[1;32m    516\u001b[0m         v2_i_tokens,\n\u001b[1;32m    517\u001b[0m         v2_q_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[1;32m    523\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/workspace/data/dataset.py:262\u001b[0m, in \u001b[0;36mVQADataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    259\u001b[0m idx_tr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data_item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    260\u001b[0m label \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(idx_tr, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_classes\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 262\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_item\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m### Debug ###\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# image.show()\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# print('Question:', data_item['question'])\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m###-------###\u001b[39;00m\n\u001b[1;32m    269\u001b[0m i_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    876\u001b[0m ):\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/workspace')\n",
    "\n",
    "from trainer import *\n",
    "\n",
    "cfg = {\n",
    "    ### META ###\n",
    "    'name': 'DANN',\n",
    "\n",
    "\n",
    "    ### DataLoader ###\n",
    "    'n_classes': 10,\n",
    "    'v2_samples_per_answer': 300,\n",
    "    'abs_samples_per_answer': 150,\n",
    "    'source_domain': 'abs',\n",
    "    \n",
    "    ### VLModel ###\n",
    "    'image_encoder': 'facebook/dinov2-base',\n",
    "    'text_encoder': 'bert-base-uncased',\n",
    "    \n",
    "    ## Embedder\n",
    "    'num_attn_heads': 8,\n",
    "    'fusion_mode': 'cat',\n",
    "    'num_stacked_attn': 1, \n",
    "    \n",
    "    'criss_cross__drop_p': 0.0,\n",
    "    'post_concat__drop_p': 0.0, \n",
    "    'embed_attn__add_residual': False,\n",
    "    'embed_attn__drop_p': 0.0,\n",
    "\n",
    "    ## Label Classifier\n",
    "    'label_classifier__use_bn': False,\n",
    "    'label_classifier__drop_p': 0.0,\n",
    "    'label_classifier__repeat_layers': [0, 0], \n",
    "\n",
    "    ## Domain Classifier\n",
    "    'domain_classifier__use_bn': True,\n",
    "    'domain_classifier__drop_p': 0.5,\n",
    "    'domain_classifier__repeat_layers': [0, 0], \n",
    "\n",
    "    ### Objective ###\n",
    "    # loss fn\n",
    "    'domain_adaptation_method': 'domain_adversarial',  # 'naive', 'importance_sampling', 'domain_adversarial'\n",
    "\n",
    "    ### Trainer ###\n",
    "    'relaxation_period': 3,  # epochs to wait where accuracy is dropping \n",
    "                             # below moving average before ending the run\n",
    "                             # (-1 to disable it)\n",
    "    \n",
    "    'batch_size': 100,\n",
    "    'epochs': 10,\n",
    "    'base_lr': 0.001,\n",
    "    'weight_decay': 5e-4,\n",
    "\n",
    "    ### Logging ###\n",
    "    'print_logs': True,\n",
    "    'show_plot': True,\n",
    "    'weights_save_root': './weights/raw'\n",
    "}\n",
    "\n",
    "\n",
    "if True:\n",
    "    # v2\n",
    "    cfg['source_domain'] = 'v2'\n",
    "    trainer = DA_Trainer(cfg, vqa_v2, vqa_abs)\n",
    "    v2_ckpt_path = cfg['weights_save_path']\n",
    "\n",
    "    trainer.train(show_plot=True)\n",
    "\n",
    "    # abs\n",
    "    cfg['source_domain'] = 'abs'\n",
    "    trainer = DA_Trainer(cfg, vqa_v2, vqa_abs)\n",
    "    abs_ckpt_path = cfg['weights_save_path']\n",
    "\n",
    "    trainer.train(show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2_ckpt_path = './weights/raw/name=DANN__n_classes=10__v2_samples_per_answer=300__abs_samples_per_answer=150__source_domain=v2__base_lr=0.001__domain_adaptation_method=domain_adversarial__.pth'\n",
    "\n",
    "# abs_ckpt_path = './weights/raw/name=DANN__n_classes=10__v2_samples_per_answer=300__abs_samples_per_answer=150__source_domain=abs__base_lr=0.001__domain_adaptation_method=domain_adversarial__.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['v2_ckpt'] = v2_ckpt_path\n",
    "cfg['abs_ckpt'] = abs_ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(v2_train_data, v2_val_data), (abs_train_data, abs_val_data), labels = data_processing_v2(cfg, vqa_v2, vqa_abs)\n",
    "model = VLModel(cfg, return_embeddings=True).cuda()\n",
    "\n",
    "def eval_domain_adaptation(eval_dataset, model_ckpt, evaluate_train=False):\n",
    "    if eval_dataset == 'v2':\n",
    "        train_data, val_data = v2_train_data, v2_val_data\n",
    "    elif eval_dataset == 'abs':\n",
    "        train_data, val_data = abs_train_data, abs_val_data\n",
    "\n",
    "    train_dataset = VQADataset(cfg, train_data)\n",
    "    val_dataset = VQADataset(cfg, val_data)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=cfg['batch_size'], shuffle=True)\n",
    "\n",
    "    if model_ckpt == 'v2':\n",
    "        ckpt = cfg['v2_ckpt']\n",
    "    elif model_ckpt == 'abs':\n",
    "        ckpt = cfg['abs_ckpt']\n",
    "\n",
    "    state_dict = torch.load(ckpt, weights_only=True)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    dataloader = val_dataloader if not evaluate_train else train_dataloader\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    embeddings = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_tokens, q_tokens, label in dataloader:\n",
    "            i_tokens = {key: value.cuda() for key, value in i_tokens.items()}\n",
    "            q_tokens = {key: value.cuda() for key, value in q_tokens.items()}\n",
    "            label = label.cuda()\n",
    "            \n",
    "            logits, embedding = model(i_tokens, q_tokens)\n",
    "            embeddings = np.concatenate((embeddings, embedding), axis=0) if embeddings is not None else embedding\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted_indices = torch.max(logits, dim=1)\n",
    "            label_indices = torch.argmax(label, dim=1)\n",
    "            is_correct = (predicted_indices == label_indices)\n",
    "\n",
    "            total += label.shape[0]\n",
    "            correct += is_correct.sum().item()\n",
    "\n",
    "    accuracy = (correct / total)\n",
    "\n",
    "    source = 'v2 ' if model_ckpt == 'v2' else 'abs'\n",
    "    target = 'v2 ' if eval_dataset == 'v2' else 'abs'\n",
    "    split = 'train set' if evaluate_train else 'val set  '\n",
    "    print(f'{source} -> {target} | {split} \\t accuracy = {accuracy*100:.2f}%')\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 -> v2\n",
    "v2_val_embeddings = eval_domain_adaptation(model_ckpt='v2', eval_dataset='v2', evaluate_train=False)\n",
    "v2_train_embeddings = eval_domain_adaptation(model_ckpt='v2', eval_dataset='v2', evaluate_train=True)\n",
    "\n",
    "# abs -> abs\n",
    "abs_val_embeddings = eval_domain_adaptation(model_ckpt='abs', eval_dataset='abs', evaluate_train=False)\n",
    "abs_train_embeddings = eval_domain_adaptation(model_ckpt='abs', eval_dataset='abs', evaluate_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 -> abs\n",
    "v2_abs_val_embeddings = eval_domain_adaptation(model_ckpt='v2', eval_dataset='abs', evaluate_train=False)\n",
    "v2_abs_train_embeddings = eval_domain_adaptation(model_ckpt='v2', eval_dataset='abs', evaluate_train=True)\n",
    "\n",
    "# abs -> v2\n",
    "abs_v2_val_embeddings = eval_domain_adaptation(model_ckpt='abs', eval_dataset='v2', evaluate_train=False)\n",
    "abs_v2_train_embeddings = eval_domain_adaptation(model_ckpt='abs', eval_dataset='v2', evaluate_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_tsne(embeddings, plot_labels, perplexities, title=''):\n",
    "    n_plots = len(perplexities)\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(6 * n_plots, 6))\n",
    "    \n",
    "    for idx, perplexity in enumerate(perplexities):\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "        tsne_embeddings = tsne.fit_transform(embeddings)\n",
    "        \n",
    "        colors = {'vv': 'blue', 'aa': 'red', 'va': 'orange', 'av': 'violet'}\n",
    "        ax = axes[idx]\n",
    "        for i, plot_label in enumerate(plot_labels):\n",
    "                \n",
    "            ax.scatter(tsne_embeddings[i, 0], tsne_embeddings[i, 1], color=colors[plot_label])\n",
    "            ax.text(tsne_embeddings[i, 0] + 0.1, tsne_embeddings[i, 1], '', fontsize=9)\n",
    "        \n",
    "        ax.set_title(f'Perplexity = {perplexity}')\n",
    "    \n",
    "    custom_legend = [mpatches.Patch(color=colors[label], label=label) for label in set(plot_labels)]\n",
    "    fig.legend(handles=custom_legend)\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_v2 = len(v2_val_embeddings)\n",
    "n_abs = len(abs_val_embeddings)\n",
    "\n",
    "# v2 -> v2\n",
    "plot_tsne(v2_val_embeddings, ['vv'] * n_v2, perplexities=[2,4,8,16,32], title='v2->v2 (val set)')\n",
    "plot_tsne(v2_train_embeddings, ['vv'] * n_v2, perplexities=[2,4,8,16,32], title='v2->v2 (train set)')\n",
    "\n",
    "# abs -> abs\n",
    "plot_tsne(abs_val_embeddings, ['aa'] * n_abs, perplexities=[2,4,8,16,32], title='abs->abs (val set)')\n",
    "plot_tsne(abs_train_embeddings, ['aa'] * n_abs, perplexities=[2,4,8,16,32], title='abs->abs (train set)')\n",
    "\n",
    "# combined plot\n",
    "val_embeddings = np.concatenate((v2_val_embeddings, abs_val_embeddings), axis=0)\n",
    "plot_tsne(val_embeddings, ['vv'] * n_v2 + ['aa'] * n_abs, perplexities=[2,4,8,16,32], title='Same Domain (val set)')\n",
    "\n",
    "train_embeddings = np.concatenate((v2_train_embeddings, abs_train_embeddings), axis=0)\n",
    "plot_tsne(train_embeddings, ['vv'] * n_v2 + ['aa'] * n_abs, perplexities=[2,4,8,16,32], title='Same Domain (train set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 -> abs\n",
    "plot_tsne(v2_abs_val_embeddings, ['va'] * n_abs, perplexities=[2,4,8,16,32], title='v2->abs (val set)')\n",
    "plot_tsne(v2_abs_train_embeddings, ['va'] * n_abs, perplexities=[2,4,8,16,32], title='v2->abs (train set)')\n",
    "\n",
    "# abs -> v2\n",
    "plot_tsne(abs_v2_val_embeddings, ['av'] * n_v2, perplexities=[2,4,8,16,32], title='abs->v2 (val set)')\n",
    "plot_tsne(abs_v2_train_embeddings, ['av'] * n_v2, perplexities=[2,4,8,16,32], title='abs->v2 (train set)')\n",
    "\n",
    "# combined plot\n",
    "val_embeddings = np.concatenate((v2_abs_val_embeddings, abs_v2_val_embeddings), axis=0)\n",
    "plot_tsne(val_embeddings, ['va'] * n_abs + ['av'] * n_v2, perplexities=[2,4,8,16,32], title='Cross Domain (val set)')\n",
    "\n",
    "train_embeddings = np.concatenate((v2_abs_train_embeddings, abs_v2_train_embeddings), axis=0)\n",
    "plot_tsne(train_embeddings, ['va'] * n_abs + ['av'] * n_v2, perplexities=[2,4,8,16,32], title='Cross Domain (train set)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
