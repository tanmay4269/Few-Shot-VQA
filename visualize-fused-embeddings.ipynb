{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Better Multi-Modal Fused Embeddings of Image and Question Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataset, image_encoder, text_encoder):\n",
    "        if dataset == 'v2':\n",
    "            self.image_root = 'data/vqa-v2/val2014/val2014/COCO_val2014_000000'\n",
    "            data_path = 'data/vqa-v2/v2_OpenEnded_mscoco_val2014_questions.json'            \n",
    "        elif dataset == 'abs':\n",
    "            self.image_root = 'data/vqa-abstract/img_train/abstract_v002_train2015_0000000'\n",
    "            data_path = 'data/vqa-abstract/questions_train/OpenEnded_abstract_v002_train2015_questions.json'\n",
    "\n",
    "        with open(data_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.data = data['questions']\n",
    "\n",
    "        self.i_processor = AutoImageProcessor.from_pretrained(image_encoder)\n",
    "        self.q_tokenizer = BertTokenizer.from_pretrained(text_encoder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = self.data[idx]\n",
    "        image, question = self.get_data(data_item)\n",
    "        \n",
    "        i_tokens = self.i_processor(images=image, return_tensors='pt')\n",
    "        q_tokens = self.q_tokenizer(question, return_tensors='pt')\n",
    "\n",
    "        # dirty way to fix dimention issue:\n",
    "        i_tokens['pixel_values'] = i_tokens['pixel_values'].squeeze(0)\n",
    "\n",
    "        for key, value in q_tokens.items():\n",
    "            q_tokens[key] = value.squeeze(0)\n",
    "\n",
    "        return i_tokens, q_tokens \n",
    "    \n",
    "    def get_data(self, data):\n",
    "        image_id = data['image_id']\n",
    "\n",
    "        if self.dataset == 'v2':\n",
    "            image_id = str(image_id).zfill(6)\n",
    "        elif self.dataset == 'abs':\n",
    "            image_id = str(image_id).zfill(5)\n",
    "\n",
    "        image_path = f'{self.image_root}{image_id}.jpg'\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        return image, data['question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLM(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, fusion_mode, embed_dim=768):\n",
    "        super(VLM, self).__init__()\n",
    "\n",
    "        self.fusion_mode = fusion_mode\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.i_encoder = AutoModel.from_pretrained(image_encoder)\n",
    "        self.q_encoder = BertModel.from_pretrained(text_encoder)\n",
    "\n",
    "        attn_heads = 4\n",
    "        attn_embed_dim = 768\n",
    "        self.attn_q2i = nn.MultiheadAttention(attn_embed_dim, attn_heads, batch_first=True)\n",
    "        self.attn_i2q = nn.MultiheadAttention(attn_embed_dim, attn_heads, batch_first=True)\n",
    "        \n",
    "        self.attn_i = nn.MultiheadAttention(attn_embed_dim, attn_heads, batch_first=True)\n",
    "        self.attn_q = nn.MultiheadAttention(attn_embed_dim, attn_heads, batch_first=True)\n",
    "\n",
    "        if fusion_mode == 'cat': attn_embed_dim *= 2\n",
    "        self.attn_e = nn.MultiheadAttention(attn_embed_dim, attn_heads, batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, i_tokens, q_tokens):\n",
    "        with torch.no_grad():\n",
    "            i_embeddings = self.i_encoder(**i_tokens).last_hidden_state\n",
    "            q_embeddings = self.q_encoder(**q_tokens).last_hidden_state\n",
    "\n",
    "        i_attended, _ = self.attn_q2i(i_embeddings, q_embeddings, q_embeddings)\n",
    "        q_attended, _ = self.attn_i2q(q_embeddings, i_embeddings, i_embeddings)\n",
    "\n",
    "        i_embeddings = torch.zeros((i_attended.shape[0], 1, self.embed_dim))\n",
    "        i_embeddings, _ = self.attn_i(i_embeddings, i_attended, i_attended)\n",
    "\n",
    "        q_embeddings = torch.zeros((q_attended.shape[0], 1, self.embed_dim))\n",
    "        q_embeddings, _ = self.attn_q(q_embeddings, q_attended, q_attended)\n",
    "\n",
    "        if self.fusion_mode == 'cat':\n",
    "            embedding = torch.cat((i_embeddings, q_embeddings), dim=-1)  # along channels\n",
    "        elif self.fusion_mode == 'cat_v2':\n",
    "            i_embeddings = i_embeddings.reshape(i_embeddings.shape[0], -1, self.embed_dim // 2)\n",
    "            q_embeddings = q_embeddings.reshape(q_embeddings.shape[0], -1, self.embed_dim // 2)\n",
    "            embedding = torch.cat((i_embeddings, q_embeddings), dim=-1)\n",
    "        elif self.fusion_mode == 'mult':\n",
    "            embedding = i_embeddings * q_embeddings\n",
    "        elif self.fusion_mode == 'add':\n",
    "            embedding = i_embeddings + q_embeddings\n",
    "\n",
    "        zeros = torch.zeros((embedding.shape[0], 1, self.embed_dim))\n",
    "        embedding, _ = self.attn_e(zeros, embedding, embedding)\n",
    "\n",
    "        return embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 768])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "image_encoder = 'facebook/dinov2-base'\n",
    "text_encoder = 'bert-base-uncased'\n",
    "\n",
    "v2_dataset = VQADataset('v2', image_encoder, text_encoder)\n",
    "v2_dataloader = DataLoader(v2_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model = VLM(image_encoder, text_encoder, fusion_mode='cat_v2')\n",
    "\n",
    "for i_tokens, q_tokens in v2_dataloader:\n",
    "    embedding = model(i_tokens, q_tokens)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
